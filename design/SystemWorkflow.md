# System Workflow

## Tutorial Generator
See [TutorialViewModel->generate](/api/synapi/views.py). 
Currently implemented by prompt engineering with GPT, the system first create a message based on the question and appropriate context to generate a tutorial. 
The tutorial will be returned in json format, parsed to match our model definition and stored in db. A lot should be improved for this system. We face problems 
such as tutorial generated by gpt is not up to date with the newest website and sometimes the tutorial itself does not make much sense or the tutorial does not 
contain enough information for a complete tutorial to be presented, and in most cases produces extra steps where unecessary. 

## Tutorial
See [api/synapi/models](api/synapi/models/tutorial.py) for model definition. The models are equally generated and accessible in the frontend. 
See (open-api_generate)[scripts/open-api_generate.ps1]. 

## Element Label
At the time of writing, the target element is represented by a human readable description (eg. "The Commit changes... button") in the system. The reason for the decision is that 
GPT is a NLP model, it works the best working with human languages. This brings chanlleges to the following subsystems because the target element representation is 
essential in the locating of the actual element in the UI, since a human language to machine language mapping is required. An alternative here is to provide gpt with 
enough context (i.e. including the structural information of the site, preferably the entire html doc) so that it can create an accurate css selector as the target 
element representation. However, per low-level testing, it seems impossible for gpt to handle text information as large as a html document for a complex application. 

## Element Tree and Visable Element List
The implementation of this is based upon the Element Label. If the Element Lable is a css selector, the element tree will correspond to the DOM of a webpage and the list 
of element will be easily accessible through the built-in querySelector. 

However, a css selector is often impossible to be determined and remain vital for any application lifecycle. First of all, it is hard for gpt to know the correct 
css selector as a closed system. Secondly, css selectors are not under our control and can change on every software update. 

A proposed solution and partially working solution is to use the [AOM](https://wicg.github.io/aom/spec/). With AOM, we allow for the possibility of using human readable description 
as element label. The consideration is that different from css selector, the accessibility of a web page is less likely to change significantly change in a short period 
of time. No matter where a button is located in the page, it should have a text description or an aria-label following the aria standard. Despict the seemingly promissing, 
features of AOM, the problem that lies with it is that chrome did not fully finish or published the AOM api. The api is only accessible through the dev versions of chrome 
which does not meet the marketing purposes. Nonetheless, this is not the end of use of AOM. One of the first thing to realize is that AOM is built upon DOM, all features 
that are provided by Chrome should be implementable with effort. 

## Element Matcher
Uses language models to find in a list of node the node that matches the element label most. 

It is possible for gpt to describe a link as a button (e.g. The Element label "The Commit Changes... button" might show up in AOM as Link Commit changes...). 
The purpose of this module is to find the best node in the page that matches gpt generated description.

See [GetMatchingElement](api/ai)


